version: "3.3"
services:

  mbaza-tts-kin-gpu:
    container_name: mbaza-tts-kin-gpu
    restart: unless-stopped
    env_file:
      - .env
    build:
      context: ./
      dockerfile: Dockerfile-kin-gpu
      labels:
        - mbaza-tts-kin-gpu
    ports:
      - "6904:6904"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ '0' ] # Defaults to first gpu, it should be "1" for a server with second gpu.
              capabilities: [ gpu ]
    environment:
      - THREADS=2
      - PROCESSES=1 # allow 2250 MB /x1 worker
      - TIMEOUT=240 # worker timeout
      - GRACEFUL_TIMEOUT=220 # worker timeout
      - SRV_HOST=0.0.0.0
      - SRV_PORT=6904
      - APP_LANG=kin
      - APP_SENTENCE_MAX_WORDS=40
      - APP_SENTENCE_INFER_BATCH_SIZE=3 # allow around 550MB /x1 APP_SENTENCE_INFER_BATCH_SIZE
      - APP_SAMPLE_TEXT=. indi nkuru wasoma ni uko muri suwede urukiko rwategetse gufunga burundu mbanenande kuko yahamwe n'ibyaha bya jenoside .
      - APP_LOG_LEVEL=debug

#  mbaza-tts-caddy:
#    image: caddy:2-alpine
#    container_name: mbaza-tts-caddy
#    restart: unless-stopped
#    env_file:
#      - .env
#    ports:
#      - "5002:5002"
#    environment:
#      - PROXY_TARGET=mbaza-tts-kin-gpu:6904
#    volumes:
#      - $PWD/Caddyfile:/etc/caddy/Caddyfile
#      - caddy_data:/data
#      - caddy_config:/config
#    depends_on:
#      - mbaza-tts-kin-gpu
#
#volumes:
#  caddy_data:
#    external: true
#  caddy_config:
